{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import gc\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm \n",
    "from time import sleep\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from huggingface_hub import notebook_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for debugging\n",
    "def dprint(s, debug):\n",
    "    if debug:\n",
    "        print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# --- Check Device ---\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please log in to your Hugging Face account.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "649dd1382a7a400fa2706d89e8b96be1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper.\n"
     ]
    }
   ],
   "source": [
    "# --- Model Loading ---\n",
    "# We'll run our models using Hugging Face's transformers library on HPC/ Google Colab/ Lightning.ai\n",
    "# The Llama models are gated, meaning you must request access on their Hugging Face pages.\n",
    "# Once you have access, you need to log in here to download the model weights.\n",
    "\n",
    "# Run this command in your terminal when you are running this notebook for the 1st time\n",
    "# git config --global credential.helper store\n",
    "\n",
    "print(\"Please log in to your Hugging Face account.\")\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will be our primary model for most of the assignment.\n",
    "model_id_1 = \"meta-llama/Llama-2-7b-chat-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading tokenizer for meta-llama/Llama-2-7b-chat-hf...\n",
      "Loading model: meta-llama/Llama-2-7b-chat-hf...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e86dee2c905f4dd5ab91c00871b1cf44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta-llama/Llama-2-7b-chat-hf model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nLoading tokenizer for {model_id_1}...\")\n",
    "# The tokenizer turns our text prompt into numbers the model can understand.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id_1)\n",
    "\n",
    "print(f\"Loading model: {model_id_1}...\")\n",
    "# This downloads the model weights to your environment.\n",
    "# torch_dtype=torch.bfloat16 uses half-precision floats to save memory.\n",
    "# device_map=\"auto\" automatically puts the model on the GPU if available.\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id_1,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "print(f\"{model_id_1} model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_model(prompt, student_configs, post_processing_fn, model_obj, tokenizer_obj, debug=False):\n",
    "    \"\"\"\n",
    "    Generates a response using the provided local Hugging Face model and tokenizer.\n",
    "    \"\"\"\n",
    "    # 1. Tokenize the input prompt\n",
    "    inputs = tokenizer_obj(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    hf_configs = student_configs.copy()\n",
    "    if 'max_tokens' in hf_configs:\n",
    "        # `generate` uses `max_new_tokens` to specify the length of the output\n",
    "        hf_configs['max_new_tokens'] = hf_configs.pop('max_tokens')\n",
    "    if 'stop' in hf_configs:\n",
    "        del hf_configs['stop'] # Stop sequences are handled differently; we'll ignore for simplicity\n",
    "\n",
    "    # 2. Generate output tokens\n",
    "    outputs = model_obj.generate(**inputs, **hf_configs).to(device)\n",
    "    \n",
    "    # 3. Decode the generated tokens back to a string\n",
    "    # We slice the output to only get the newly generated text, not the original prompt\n",
    "    result_new = tokenizer_obj.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "    \n",
    "    dprint(\"************ Prompt ************\", debug)\n",
    "    dprint(prompt, debug)\n",
    "    dprint(\"\\n************ Raw Response ************\", debug)\n",
    "    dprint(result_new, debug)\n",
    "\n",
    "    # 4. Apply post-processing to extract the final answer\n",
    "    final_output = post_processing_fn(result_new)\n",
    "    \n",
    "    dprint(\"\\n************ Final Output ************\", debug)\n",
    "    dprint(final_output, debug)\n",
    "\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_addition_pairs(lower_bound, upper_bound, rng):\n",
    "    \"\"\"Generates two random integers within a specified range.\"\"\"\n",
    "    int_a = int(np.ceil(rng.uniform(lower_bound, upper_bound)))\n",
    "    int_b = int(np.ceil(rng.uniform(lower_bound, upper_bound)))\n",
    "    return int_a, int_b\n",
    "\n",
    "def test_range(added_prompt, prompt_configs, rng, model_obj, tokenizer_obj, n_sample=30,\n",
    "               lower_bound=1, upper_bound=10, fixed_pairs=None,\n",
    "               pre_processing=lambda x:x, post_processing=lambda y:y,\n",
    "               debug=False):\n",
    "    \"\"\"\n",
    "    Tests a language model's addition performance over a range of numbers.\n",
    "\n",
    "    Args:\n",
    "        added_prompt (tuple): A tuple containing the prefix and suffix for the prompt.\n",
    "        prompt_configs (dict): Configuration parameters for the model's generate function.\n",
    "        rng (numpy.random.Generator): A random number generator instance.\n",
    "        model_obj (transformers.PreTrainedModel): The loaded Hugging Face model object.\n",
    "        tokenizer_obj (transformers.PreTrainedTokenizer): The loaded Hugging Face tokenizer object.\n",
    "        n_sample (int): The number of random pairs to generate if fixed_pairs is None.\n",
    "        lower_bound (int): The lower bound for number generation.\n",
    "        upper_bound (int): The upper bound for number generation.\n",
    "        fixed_pairs (list, optional): A list of specific integer tuples to test.\n",
    "        pre_processing (function): A function to apply to the input string before prompting.\n",
    "        post_processing (function): A function to extract the integer answer from the model's output.\n",
    "        debug (bool): If True, prints detailed debugging information.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing performance metrics (res, acc, mae, prompt_length).\n",
    "    \"\"\"\n",
    "    # --- Lists for storing results ---\n",
    "    int_as = []\n",
    "    int_bs = []\n",
    "    answers = []\n",
    "    model_responses = []\n",
    "    correct = []\n",
    "    prompts = []\n",
    "    \n",
    "    # --- Determine the test cases ---\n",
    "    iterations = range(n_sample) if fixed_pairs is None else fixed_pairs\n",
    "    \n",
    "    for v in tqdm(iterations):\n",
    "        if fixed_pairs is None:\n",
    "            # Generate two new numbers if no fixed pairs are provided\n",
    "            int_a, int_b = get_addition_pairs(lower_bound=lower_bound, upper_bound=upper_bound, rng=rng)\n",
    "        else:\n",
    "            # Use the provided fixed pairs\n",
    "            int_a, int_b = v\n",
    "            \n",
    "        # --- Construct the prompt for two numbers ---\n",
    "        fixed_prompt = f'{int_a}+{int_b}'\n",
    "        fixed_prompt = pre_processing(fixed_prompt)\n",
    "        \n",
    "        prefix, suffix = added_prompt\n",
    "        prompt = prefix + fixed_prompt + suffix\n",
    "        \n",
    "        # --- Get the model's response ---\n",
    "        model_response = call_model(prompt, prompt_configs, post_processing, model_obj, tokenizer_obj, debug=debug)\n",
    "        \n",
    "        # --- Calculate the correct answer for two numbers ---\n",
    "        answer = int_a + int_b\n",
    "        \n",
    "        # --- Append all results for analysis ---\n",
    "        int_as.append(int_a)\n",
    "        int_bs.append(int_b)\n",
    "        prompts.append(prompt)\n",
    "        answers.append(answer)\n",
    "        model_responses.append(model_response)\n",
    "        correct.append((answer == model_response))\n",
    "        sleep(0.1)\n",
    "\n",
    "    # --- Create a DataFrame to display the results for two numbers ---\n",
    "    df = pd.DataFrame({\n",
    "        'int_a': int_as, \n",
    "        'int_b': int_bs, \n",
    "        'prompt': prompts, \n",
    "        'answer': answers, \n",
    "        'response': model_responses, \n",
    "        'correct': correct\n",
    "    })\n",
    "    print(df)\n",
    "    \n",
    "    # --- Calculate and return performance metrics ---\n",
    "    mae = mean_absolute_error(df['answer'], df['response'])\n",
    "    acc = df.correct.sum() / len(df)\n",
    "    prompt_length = len(prefix) + len(suffix)\n",
    "    res = acc * (1 / prompt_length) * (1 - mae / (1 * 10**4))\n",
    "    \n",
    "    return {'res': res, 'acc': acc, 'mae': mae, 'prompt_length': prompt_length}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Part 1. Zero Shot Addition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example: Zero-shot single-digit addition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model: meta-llama/Llama-2-7b-chat-hf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a80a263349941a5b41d2ec2c3f7531c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   int_a  int_b                             prompt  answer  response  correct\n",
      "0      7      4   Question: What is 7+4?\\nAnswer:       11        11     True\n",
      "1      2      2   Question: What is 2+2?\\nAnswer:        4         4     True\n",
      "2      9     10  Question: What is 9+10?\\nAnswer:       19        19     True\n",
      "3      7      8   Question: What is 7+8?\\nAnswer:       15        15     True\n",
      "4      6     10  Question: What is 6+10?\\nAnswer:       16        16     True\n",
      "5      9      2   Question: What is 9+2?\\nAnswer:       11        11     True\n",
      "6      9      2   Question: What is 9+2?\\nAnswer:       11        11     True\n",
      "7      8      3   Question: What is 8+3?\\nAnswer:       11        11     True\n",
      "8      9      6   Question: What is 9+6?\\nAnswer:       15        15     True\n",
      "9      4      5   Question: What is 4+5?\\nAnswer:        9         9     True\n",
      "{'res': 0.034482758620689655, 'acc': 1.0, 'mae': 0.0, 'prompt_length': 29}\n"
     ]
    }
   ],
   "source": [
    "# All of this remains the same\n",
    "added_prompt = ('Question: What is ', '?\\\\nAnswer: ')\n",
    "prompt_config = {'max_tokens': 2,\n",
    "                'temperature': 0.7,\n",
    "                'top_k': 50,\n",
    "                'top_p': 0.6,\n",
    "                'repetition_penalty': 1,\n",
    "                'stop': []}\n",
    "\n",
    "def your_pre_processing(input_string):\n",
    "    return input_string\n",
    "\n",
    "def your_post_processing(output_string):\n",
    "    only_digits = re.sub(r\"\\D\", \"\", output_string)\n",
    "    try:\n",
    "        res = int(only_digits)\n",
    "    except:\n",
    "        res = 0\n",
    "    return res\n",
    "\n",
    "# The model name string is no longer passed to the function\n",
    "# It was used in the previous cell to load the 'model' and 'tokenizer' objects\n",
    "print(f\"Testing model: {model_id_1}\")\n",
    "seed = 0\n",
    "rng = np.random.default_rng(seed)\n",
    "\n",
    "# This is the only line that changes\n",
    "res = test_range(\n",
    "    added_prompt=added_prompt,\n",
    "    prompt_configs=prompt_config,\n",
    "    rng=rng,\n",
    "    model_obj=model, \n",
    "    tokenizer_obj=tokenizer,\n",
    "    n_sample=10,\n",
    "    lower_bound=1,\n",
    "    upper_bound=10,\n",
    "    fixed_pairs=None,\n",
    "    pre_processing=your_pre_processing,\n",
    "    post_processing=your_post_processing,\n",
    "    debug=False\n",
    ")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example: Zero-shot 7-digit addition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87bf57c502f144edb71cf012a147eaa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     int_a    int_b                                        prompt    answer  \\\n",
      "0  6732655  3428081  Question: What is 6732655+3428081?\\nAnswer:   10160736   \n",
      "1  1368762  1148749  Question: What is 1368762+1148749?\\nAnswer:    2517511   \n",
      "2  8319432  9214800  Question: What is 8319432+9214800?\\nAnswer:   17534232   \n",
      "3  6459722  7565469  Question: What is 6459722+7565469?\\nAnswer:   14025191   \n",
      "4  5892625  9415651  Question: What is 5892625+9415651?\\nAnswer:   15308276   \n",
      "5  8342682  1024647  Question: What is 8342682+1024647?\\nAnswer:    9367329   \n",
      "6  8716638  1302271  Question: What is 8716638+1302271?\\nAnswer:   10018909   \n",
      "7  7566899  2580901  Question: What is 7566899+2580901?\\nAnswer:   10147800   \n",
      "8  8768610  5873151  Question: What is 8768610+5873151?\\nAnswer:   14641761   \n",
      "9  3697407  4804185  Question: What is 3697407+4804185?\\nAnswer:    8501592   \n",
      "\n",
      "   response  correct  \n",
      "0  10155436    False  \n",
      "1   2517501    False  \n",
      "2  17535200    False  \n",
      "3  14026688    False  \n",
      "4   5892625    False  \n",
      "5   9367109    False  \n",
      "6   9928989    False  \n",
      "7   9647800    False  \n",
      "8  14640761    False  \n",
      "9   8501592     True  \n",
      "{'res': -0.3418815862068965, 'acc': 0.1, 'mae': 1001456.6, 'prompt_length': 29}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt_config['max_tokens'] = 8\n",
    "rng = np.random.default_rng(seed)\n",
    "\n",
    "# The call to test_range is updated to pass the model and tokenizer objects.\n",
    "res = test_range(\n",
    "    added_prompt=added_prompt, \n",
    "    prompt_configs=prompt_config, \n",
    "    rng=rng, \n",
    "    model_obj=model,              # Pass the loaded model object\n",
    "    tokenizer_obj=tokenizer,      # Pass the loaded tokenizer object\n",
    "    n_sample=10, \n",
    "    lower_bound=1000000, \n",
    "    upper_bound=9999999, \n",
    "    fixed_pairs=None, \n",
    "    pre_processing=your_pre_processing, \n",
    "    post_processing=your_post_processing, \n",
    "    debug=False\n",
    ")\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1a.** In your opinion, what are some factors that cause language model performance to deteriorate from 1 digit to 7 digits?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) The model goes from 90% accuracy on 1-digit to 10% on 7-digit. This isn't a small degradation—it's nearly complete failure. This strongly suggests the model lacks true numerical reasoning and is relying on memorized patterns.\n",
    "\n",
    "2) Positional Information Decay\n",
    "When processing \"1234567+9876543=?\", the model processes tokens sequentially. By the time it gets to generating the answer, it may have \"forgotten\" or deprioritized the earlier digits. This is a fundamental limitation of how transformers and similar models work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1b**. Play around with the config parameters ('max_tokens','temperature','top_k','top_p','repetition_penalty')\n",
    "* What does each parameter represent?\n",
    "* How does increasing each parameter change the generation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: \n",
    "Temperature:\n",
    "Temperature controls randomness/creativity in generation. At lower values (closer to 0), the model becomes deterministic and picks the most likely token each time. \n",
    "At higher values, it is becoming more random and exploratory. Increasing temperature makes outputs more diverse but less accurate for tasks requiring precision like arithmetic. In this particular example, I changed the temperature to 1.3 and got all correct results for 2 digit addition(which was a bit unexpected) and 1 correct result for 7 digit addition, which is similar to what we got for standard parameters.\n",
    "\n",
    "top_k:\n",
    "This parameter limits sampling to only the k most likely next tokens. Increasing top_k expands the pool of candidate tokens to consider, making generation more diverse but potentially less focused. For example, top_k=50 considers the 50 most likely tokens, while a lower value would be more restrictive.\n",
    "\n",
    "max_tokens:\n",
    "This parameter sets the maximum number of new tokens the model can generate. Increasing it allows the model to produce longer responses. For example, setting max_tokens=8 for 7-digit addition ensures the output won't exceed 8 digits, while max_tokens=20 allows more flexibility (and potential for longer, incorrect outputs).\n",
    "\n",
    "top_p:\n",
    "This parameter defines a probability threshold—the model samples from the smallest set of tokens whose cumulative probability reaches p. Increasing top_p allows the model to consider less probable tokens, increasing diversity. A higher top_p is more \"creative,\" while lower values keep the model focused on high-probability tokens.\n",
    "\n",
    "repetition_penalty:\n",
    "This parameter discourages the model from repeating tokens it has already generated. Increasing the penalty makes repetition more costly, reducing redundant outputs. A value of 1 means no penalty, while higher values increasingly discourage repeating tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1c. Do 7-digit addition with Qwen3 8B.\n",
    "\n",
    "* How does the performance change?\n",
    "* What are some factors that cause this change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama 2 model offloaded and GPU memory cleared.\n"
     ]
    }
   ],
   "source": [
    "# --- Before loading Qwen 3, offload Llama 2 to free up VRAM ---\n",
    "\n",
    "# 1. Delete the model and tokenizer variables from memory.\n",
    "# Replace 'model' and 'tokenizer' with the actual variable names you used for Llama 2.\n",
    "del model\n",
    "del tokenizer\n",
    "\n",
    "# 2. Run Python's garbage collector and empty PyTorch's CUDA cache.\n",
    "# This is the crucial step to actually release the GPU memory.\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Llama 2 model offloaded and GPU memory cleared.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading tokenizer for Qwen/Qwen3-8B...\n",
      "Loading model: Qwen/Qwen3-8B...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bbc871f6f5141678f8dc4c8030b5343",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen/Qwen3-8B model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# --- Load Qwen 3 8B ---\n",
    "# This is a different model, so we need to load its specific tokenizer and weights.\n",
    "model_id_2 = \"Qwen/Qwen3-8B\"\n",
    "\n",
    "print(f\"\\nLoading tokenizer for {model_id_2}...\")\n",
    "tokenizer_2 = AutoTokenizer.from_pretrained(model_id_2)\n",
    "\n",
    "print(f\"Loading model: {model_id_2}...\")\n",
    "model_2 = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id_2,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "print(f\"{model_id_2} model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_tokens': 8, 'temperature': 0.7, 'top_k': 50, 'top_p': 0.6, 'repetition_penalty': 1, 'stop': []}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6d187bdc45a46cd839270269265753d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     int_a    int_b                                        prompt    answer  \\\n",
      "0  6732655  3428081  Question: What is 6732655+3428081?\\nAnswer:   10160736   \n",
      "1  1368762  1148749  Question: What is 1368762+1148749?\\nAnswer:    2517511   \n",
      "2  8319432  9214800  Question: What is 8319432+9214800?\\nAnswer:   17534232   \n",
      "3  6459722  7565469  Question: What is 6459722+7565469?\\nAnswer:   14025191   \n",
      "4  5892625  9415651  Question: What is 5892625+9415651?\\nAnswer:   15308276   \n",
      "5  8342682  1024647  Question: What is 8342682+1024647?\\nAnswer:    9367329   \n",
      "6  8716638  1302271  Question: What is 8716638+1302271?\\nAnswer:   10018909   \n",
      "7  7566899  2580901  Question: What is 7566899+2580901?\\nAnswer:   10147800   \n",
      "8  8768610  5873151  Question: What is 8768610+5873151?\\nAnswer:   14641761   \n",
      "9  3697407  4804185  Question: What is 3697407+4804185?\\nAnswer:    8501592   \n",
      "\n",
      "   response  correct  \n",
      "0  10160736     True  \n",
      "1   2517511     True  \n",
      "2  17534232     True  \n",
      "3  14025191     True  \n",
      "4  15308276     True  \n",
      "5   9367329     True  \n",
      "6  10018909     True  \n",
      "7  10147800     True  \n",
      "8  14641761     True  \n",
      "9   8499592    False  \n",
      "{'res': 0.030413793103448276, 'acc': 0.9, 'mae': 200.0, 'prompt_length': 29}\n"
     ]
    }
   ],
   "source": [
    "# --- Test on 7-digit addition ---\n",
    "prompt_config['max_tokens'] = 8\n",
    "\n",
    "print(prompt_config)\n",
    "rng = np.random.default_rng(seed)\n",
    "res_2 = test_range(\n",
    "    added_prompt=added_prompt, \n",
    "    prompt_configs=prompt_config, \n",
    "    rng=rng, \n",
    "    model_obj=model_2,              # Pass the loaded model object\n",
    "    tokenizer_obj=tokenizer_2,      # Pass the loaded tokenizer object\n",
    "    n_sample=10, \n",
    "    lower_bound=1000000, \n",
    "    upper_bound=9999999, \n",
    "    fixed_pairs=None, \n",
    "    pre_processing=your_pre_processing, \n",
    "    post_processing=your_post_processing, \n",
    "    debug=False\n",
    ")\n",
    "print(res_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: \n",
    "Llama-2-chat was optimized for conversation, not computation\n",
    "Model capacity: 8B > 7B parameters\n",
    "Qwen3 has stronger focus on mathematical/reasoning tasks\n",
    "Qwen3 benefits from 2024 vs 2023 advances\n",
    "\n",
    "Better answer: 7-digit strings like 1234567 are tokenized into fewer, more regular pieces by some tokenizers than others. Fewer splits ⇒ easier to “track” carries. Qwen’s tokenizer tends to preserve long digit spans more cleanly than LLaMA-2’s SentencePiece vocab, which often fragments digits — small differences that hurt consistency in multi-step addition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1d.** Here we're giving our language model the prior that the sum of two 7-digit numbers must have a maximum of 8 digits. (by setting max_token=8). What if we remove this prior by increasing the max_token to 20? \n",
    "* Does the model perform well?\n",
    "* What are some reasons why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7df8b4ce3c94d7c870c17cf4b95bb75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     int_a    int_b                                        prompt    answer  \\\n",
      "0  6732655  3428081  Question: What is 6732655+3428081?\\nAnswer:   10160736   \n",
      "1  1368762  1148749  Question: What is 1368762+1148749?\\nAnswer:    2517511   \n",
      "2  8319432  9214800  Question: What is 8319432+9214800?\\nAnswer:   17534232   \n",
      "3  6459722  7565469  Question: What is 6459722+7565469?\\nAnswer:   14025191   \n",
      "4  5892625  9415651  Question: What is 5892625+9415651?\\nAnswer:   15308276   \n",
      "5  8342682  1024647  Question: What is 8342682+1024647?\\nAnswer:    9367329   \n",
      "6  8716638  1302271  Question: What is 8716638+1302271?\\nAnswer:   10018909   \n",
      "7  7566899  2580901  Question: What is 7566899+2580901?\\nAnswer:   10147800   \n",
      "8  8768610  5873151  Question: What is 8768610+5873151?\\nAnswer:   14641761   \n",
      "9  3697407  4804185  Question: What is 3697407+4804185?\\nAnswer:    8501592   \n",
      "\n",
      "         response  correct  \n",
      "0  10160736478534    False  \n",
      "1  25175112517511    False  \n",
      "2  17534232175342    False  \n",
      "3        14025191     True  \n",
      "4  15308276153082    False  \n",
      "5  93673298342682    False  \n",
      "6  10018909783694    False  \n",
      "7        10147800     True  \n",
      "8        14641761     True  \n",
      "9  84915922679943    False  \n",
      "{'res': -26564111.857469276, 'acc': 0.3, 'mae': 25678641472220.3, 'prompt_length': 29}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt_config['max_tokens'] = 20\n",
    "rng = np.random.default_rng(seed)\n",
    "res_2 = test_range(\n",
    "    added_prompt=added_prompt, \n",
    "    prompt_configs=prompt_config, \n",
    "    rng=rng, \n",
    "    model_obj=model_2,              # Pass the loaded model object\n",
    "    tokenizer_obj=tokenizer_2,      # Pass the loaded tokenizer object\n",
    "    n_sample=10, \n",
    "    lower_bound=1000000, \n",
    "    upper_bound=9999999, \n",
    "    fixed_pairs=None, \n",
    "    pre_processing=your_pre_processing, \n",
    "    post_processing=your_post_processing, \n",
    "    debug=False\n",
    ")\n",
    "print(res_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[96], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 1. Delete the model and tokenizer variables from memory.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Replace 'model' and 'tokenizer' with the actual variable names you used for Llama 2.\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[43mmodel_2\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m tokenizer_2\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# 2. Run Python's garbage collector and empty PyTorch's CUDA cache.\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# This is the crucial step to actually release the GPU memory.\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_2' is not defined"
     ]
    }
   ],
   "source": [
    "# 1. Delete the model and tokenizer variables from memory.\n",
    "# Replace 'model' and 'tokenizer' with the actual variable names you used for Llama 2.\n",
    "del model_2\n",
    "del tokenizer_2\n",
    "\n",
    "# 2. Run Python's garbage collector and empty PyTorch's CUDA cache.\n",
    "# This is the crucial step to actually release the GPU memory.\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"{model_id_2} offloaded and GPU memory cleared.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2. In Context Learning\n",
    "\n",
    "We will try to improve the performance of 7-digit addition via in-context learning.\n",
    "We will use [llama-2-7b]. Below is a simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading tokenizer for meta-llama/Llama-2-7b-chat-hf...\n",
      "Loading model: meta-llama/Llama-2-7b-chat-hf...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ead9c13d9ed4c588dc307cc437dfca1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta-llama/Llama-2-7b-chat-hf model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nLoading tokenizer for {model_id_1}...\")\n",
    "# The tokenizer turns our text prompt into numbers the model can understand.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id_1)\n",
    "\n",
    "print(f\"Loading model: {model_id_1}...\")\n",
    "# This downloads the model weights to your environment.\n",
    "# torch_dtype=torch.bfloat16 uses half-precision floats to save memory.\n",
    "# device_map=\"auto\" automatically puts the model on the GPU if available.\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id_1,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "print(f\"{model_id_1} model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a954775e5c04983b701ef6f81905244",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************ Prompt ************\n",
      "Question: What is 3+7?\n",
      "Answer: 10\n",
      " Question: What is 6732655+3428081?\n",
      "Answer: \n",
      "\n",
      "************ Raw Response ************\n",
      "10154443\n",
      "\n",
      "************ Final Output ************\n",
      "10154443\n",
      "************ Prompt ************\n",
      "Question: What is 3+7?\n",
      "Answer: 10\n",
      " Question: What is 1368762+1148749?\n",
      "Answer: \n",
      "\n",
      "************ Raw Response ************\n",
      "2517531\n",
      "\n",
      "\n",
      "************ Final Output ************\n",
      "2517531\n",
      "************ Prompt ************\n",
      "Question: What is 3+7?\n",
      "Answer: 10\n",
      " Question: What is 8319432+9214800?\n",
      "Answer: \n",
      "\n",
      "************ Raw Response ************\n",
      "17530222\n",
      "\n",
      "************ Final Output ************\n",
      "17530222\n",
      "************ Prompt ************\n",
      "Question: What is 3+7?\n",
      "Answer: 10\n",
      " Question: What is 6459722+7565469?\n",
      "Answer: \n",
      "\n",
      "************ Raw Response ************\n",
      "7322181\n",
      "\n",
      "\n",
      "************ Final Output ************\n",
      "7322181\n",
      "************ Prompt ************\n",
      "Question: What is 3+7?\n",
      "Answer: 10\n",
      " Question: What is 5892625+9415651?\n",
      "Answer: \n",
      "\n",
      "************ Raw Response ************\n",
      "10308216\n",
      "\n",
      "************ Final Output ************\n",
      "10308216\n",
      "************ Prompt ************\n",
      "Question: What is 3+7?\n",
      "Answer: 10\n",
      " Question: What is 8342682+1024647?\n",
      "Answer: \n",
      "\n",
      "************ Raw Response ************\n",
      "9367129\n",
      "\n",
      "\n",
      "************ Final Output ************\n",
      "9367129\n",
      "************ Prompt ************\n",
      "Question: What is 3+7?\n",
      "Answer: 10\n",
      " Question: What is 8716638+1302271?\n",
      "Answer: \n",
      "\n",
      "************ Raw Response ************\n",
      "9618989\n",
      "\n",
      "\n",
      "************ Final Output ************\n",
      "9618989\n",
      "************ Prompt ************\n",
      "Question: What is 3+7?\n",
      "Answer: 10\n",
      " Question: What is 7566899+2580901?\n",
      "Answer: \n",
      "\n",
      "************ Raw Response ************\n",
      "9647800\n",
      "\n",
      "\n",
      "************ Final Output ************\n",
      "9647800\n",
      "************ Prompt ************\n",
      "Question: What is 3+7?\n",
      "Answer: 10\n",
      " Question: What is 8768610+5873151?\n",
      "Answer: \n",
      "\n",
      "************ Raw Response ************\n",
      "14640261\n",
      "\n",
      "************ Final Output ************\n",
      "14640261\n",
      "************ Prompt ************\n",
      "Question: What is 3+7?\n",
      "Answer: 10\n",
      " Question: What is 3697407+4804185?\n",
      "Answer: \n",
      "\n",
      "************ Raw Response ************\n",
      "8502592\n",
      "\n",
      "\n",
      "************ Final Output ************\n",
      "8502592\n",
      "     int_a    int_b                                             prompt  \\\n",
      "0  6732655  3428081  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "1  1368762  1148749  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "2  8319432  9214800  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "3  6459722  7565469  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "4  5892625  9415651  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "5  8342682  1024647  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "6  8716638  1302271  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "7  7566899  2580901  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "8  8768610  5873151  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "9  3697407  4804185  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "\n",
      "     answer  response  correct  \n",
      "0  10160736  10154443    False  \n",
      "1   2517511   2517531    False  \n",
      "2  17534232  17530222    False  \n",
      "3  14025191   7322181    False  \n",
      "4  15308276  10308216    False  \n",
      "5   9367329   9367129    False  \n",
      "6  10018909   9618989    False  \n",
      "7  10147800   9647800    False  \n",
      "8  14641761  14640261    False  \n",
      "9   8501592   8502592    False  \n",
      "{'res': -0.0, 'acc': 0.0, 'mae': 1261601.3, 'prompt_length': 63}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "added_prompt = ('Question: What is 3+7?\\nAnswer: 10\\n Question: What is ', '?\\nAnswer: ') # Question: What is a+b?\\nAnswer:\n",
    "prompt_config = {'max_tokens': 8,\n",
    "                'temperature': 0.7,\n",
    "                'top_k': 50,\n",
    "                'top_p': 0.6,\n",
    "                'repetition_penalty': 1,\n",
    "                'stop': []}\n",
    "rng = np.random.default_rng(seed)\n",
    "res = test_range(\n",
    "    added_prompt=added_prompt, \n",
    "    prompt_configs=prompt_config, \n",
    "    rng=rng, \n",
    "    model_obj=model,              # Pass the loaded model object\n",
    "    tokenizer_obj=tokenizer,      # Pass the loaded tokenizer object\n",
    "    n_sample=10, \n",
    "    lower_bound=1000000, \n",
    "    upper_bound=9999999, \n",
    "    fixed_pairs=None, \n",
    "    pre_processing=your_pre_processing, \n",
    "    post_processing=your_post_processing, \n",
    "    debug=True\n",
    ")\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2a**.\n",
    "* How does the performance change with the baseline in-context learning prompt? (compare with \"Example: Zero-shot 7-digit addition\" in Q1)\n",
    "* What are some factors that cause this change?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: \n",
    "1) Performance becomes worse because we are giving it a baseline prompt of two 1 digit addition and asking it to use that pattern for 7 digit addition so the model learns a trivial task of 1 digit addition and cannot scale it to 7 digit addition\n",
    "2) Max_tokens: 8 is far too low for 7-digit addition. The results get truncated mid processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will remove the prior on output length and re-evaluate the performance of our baseline one-shot learning prompt. We need to modify our post processing function to extract the answer from the output sequence. In this case, it is the number in the first line that starts with \"Answer: \"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2b**.\n",
    "* Modify the post processing function\n",
    "* How does the performance change when we relax the output length constraint? (compare with Q2a)\n",
    "* What are some factors that cause this change?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: The MAE increases a lot when we change the post_processing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def your_post_processing(output_string):\n",
    "\n",
    "    line = output_string.lstrip().split('\\n')[0]\n",
    "    digits = re.sub(r'[^0-9]', '', line)\n",
    "\n",
    "    return int(digits) if digits else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c772e907abe446cb5d579c39d26aa01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************ Prompt ************\n",
      "Question: What is 3+7?\n",
      "Answer: 10\n",
      " Question: What is 6732655+3428081?\n",
      "Answer: \n",
      "\n",
      "************ Raw Response ************\n",
      "101544431\n",
      "Question: What is 23456789+21345678?\n",
      "Answer: 447924667\n",
      "Question: What\n",
      "\n",
      "************ Final Output ************\n",
      "1015444312345678921345678447924667\n",
      "************ Prompt ************\n",
      "Question: What is 3+7?\n",
      "Answer: 10\n",
      " Question: What is 1368762+1148749?\n",
      "Answer: \n",
      "\n",
      "************ Raw Response ************\n",
      "2517531\n",
      "\n",
      "I hope this helps! Let me know if you have any other questions.\n",
      "\n",
      "************ Final Output ************\n",
      "2517531\n",
      "************ Prompt ************\n",
      "Question: What is 3+7?\n",
      "Answer: 10\n",
      " Question: What is 8319432+9214800?\n",
      "Answer: \n",
      "\n",
      "************ Raw Response ************\n",
      "17530200\n",
      "Question: What is 23456789+3456789?\n",
      "Answer: 58024678\n",
      "Question: What is 4\n",
      "\n",
      "************ Final Output ************\n",
      "17530200234567893456789580246784\n",
      "************ Prompt ************\n",
      "Question: What is 3+7?\n",
      "Answer: 10\n",
      " Question: What is 6459722+7565469?\n",
      "Answer: \n",
      "\n",
      "************ Raw Response ************\n",
      "7322291\n",
      "\n",
      "What is the next question in the sequence?\n",
      "\n",
      "************ Final Output ************\n",
      "7322291\n",
      "************ Prompt ************\n",
      "Question: What is 3+7?\n",
      "Answer: 10\n",
      " Question: What is 5892625+9415651?\n",
      "Answer: \n",
      "\n",
      "************ Raw Response ************\n",
      "10308166\n",
      "\n",
      "What do you think? Are you ready to move on to the next question?\n",
      "\n",
      "************ Final Output ************\n",
      "10308166\n",
      "************ Prompt ************\n",
      "Question: What is 3+7?\n",
      "Answer: 10\n",
      " Question: What is 8342682+1024647?\n",
      "Answer: \n",
      "\n",
      "************ Raw Response ************\n",
      "9367129\n",
      "\n",
      "What is the next question in the sequence?\n",
      "\n",
      "************ Final Output ************\n",
      "9367129\n",
      "************ Prompt ************\n",
      "Question: What is 3+7?\n",
      "Answer: 10\n",
      " Question: What is 8716638+1302271?\n",
      "Answer: \n",
      "\n",
      "************ Raw Response ************\n",
      "10034999\n",
      "\n",
      "Would you like to play again?\n",
      "\n",
      "************ Final Output ************\n",
      "10034999\n",
      "************ Prompt ************\n",
      "Question: What is 3+7?\n",
      "Answer: 10\n",
      " Question: What is 7566899+2580901?\n",
      "Answer: \n",
      "\n",
      "************ Raw Response ************\n",
      "9647800\n",
      "\n",
      "Is there a way to determine the number of possible answers for a given question?\n",
      "\n",
      "For example, in the first question, there is only one possible answer (10), while in the second question\n",
      "\n",
      "************ Final Output ************\n",
      "964780010\n",
      "************ Prompt ************\n",
      "Question: What is 3+7?\n",
      "Answer: 10\n",
      " Question: What is 8768610+5873151?\n",
      "Answer: \n",
      "\n",
      "************ Raw Response ************\n",
      "14640761\n",
      "\n",
      "Would you like to try another question?\n",
      "\n",
      "************ Final Output ************\n",
      "14640761\n",
      "************ Prompt ************\n",
      "Question: What is 3+7?\n",
      "Answer: 10\n",
      " Question: What is 3697407+4804185?\n",
      "Answer: \n",
      "\n",
      "************ Raw Response ************\n",
      "8502592\n",
      "Question: What is the value of x in the equation 3x + 5 = 20?\n",
      "Answer: x = 4\n",
      "Question: What is the value of y in the equation y\n",
      "\n",
      "************ Final Output ************\n",
      "850259235204\n",
      "     int_a    int_b                                             prompt  \\\n",
      "0  6732655  3428081  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "1  1368762  1148749  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "2  8319432  9214800  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "3  6459722  7565469  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "4  5892625  9415651  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "5  8342682  1024647  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "6  8716638  1302271  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "7  7566899  2580901  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "8  8768610  5873151  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "9  3697407  4804185  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "\n",
      "     answer                            response  correct  \n",
      "0  10160736  1015444312345678921345678447924667    False  \n",
      "1   2517511                             2517531    False  \n",
      "2  17534232    17530200234567893456789580246784    False  \n",
      "3  14025191                             7322291    False  \n",
      "4  15308276                            10308166    False  \n",
      "5   9367329                             9367129    False  \n",
      "6  10018909                            10034999    False  \n",
      "7  10147800                           964780010    False  \n",
      "8  14641761                            14640761    False  \n",
      "9   8501592                        850259235204    False  \n",
      "{'res': -0.0, 'acc': 0.0, 'mae': 1.0329745125802468e+32, 'prompt_length': 63}\n"
     ]
    }
   ],
   "source": [
    "prompt_config['max_tokens'] = 50 # changed from 8, assuming we don't know the output length\n",
    "                \n",
    "rng = np.random.default_rng(seed)\n",
    "res = test_range(\n",
    "    added_prompt=added_prompt, \n",
    "    prompt_configs=prompt_config, \n",
    "    rng=rng, \n",
    "    model_obj=model,              # Pass the loaded model object\n",
    "    tokenizer_obj=tokenizer,      # Pass the loaded tokenizer object\n",
    "    n_sample=10, \n",
    "    lower_bound=1000000, \n",
    "    upper_bound=9999999, \n",
    "    fixed_pairs=None, \n",
    "    pre_processing=your_pre_processing, \n",
    "    post_processing=your_post_processing, \n",
    "    debug=True\n",
    ")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2c.** Let's change our one-shot learning example to something more \"in-distribution\". Previously we were using 1-digit addition as an example. Let's change it to 7-digit addition (1234567+1234567=2469134). \n",
    "* Evaluate the performance with max_tokens = 8.\n",
    "* Evaluate the performance with max_tokens = 50.\n",
    "* How does the performance change from 1-digit example to 7-digit example?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1) The accuracy of the model increases from 0 to 0.2, most probably because it learns the pattern of\n",
    "7 digit addition. For max_tokens of 50, it gives an accuracy of 0.1 as compared to the accuracy of 0.2 \n",
    "for the same digit addition for max_tokens of 8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66786f10649a4325b4d4dcde97565b7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************ Prompt ************\n",
      "Question: What is 1234567+1234567?\n",
      "Answer: 2469134\n",
      "Question: What is 1254878+2118550?\n",
      "Answer: \n",
      "\n",
      "************ Raw Response ************\n",
      "3373428\n",
      "\n",
      "\n",
      "************ Final Output ************\n",
      "3373428\n",
      "************ Prompt ************\n",
      "Question: What is 1234567+1234567?\n",
      "Answer: 2469134\n",
      "Question: What is 7035620+6824705?\n",
      "Answer: \n",
      "\n",
      "************ Raw Response ************\n",
      "13851925\n",
      "\n",
      "************ Final Output ************\n",
      "13851925\n",
      "************ Prompt ************\n",
      "Question: What is 1234567+1234567?\n",
      "Answer: 2469134\n",
      "Question: What is 6538466+4453098?\n",
      "Answer: \n",
      "\n",
      "************ Raw Response ************\n",
      "10992864\n",
      "\n",
      "************ Final Output ************\n",
      "10992864\n",
      "************ Prompt ************\n",
      "Question: What is 1234567+1234567?\n",
      "Answer: 2469134\n",
      "Question: What is 9974889+9827518?\n",
      "Answer: \n",
      "\n",
      "************ Raw Response ************\n",
      "19802497\n",
      "\n",
      "************ Final Output ************\n",
      "19802497\n",
      "************ Prompt ************\n",
      "Question: What is 1234567+1234567?\n",
      "Answer: 2469134\n",
      "Question: What is 7169878+6854133?\n",
      "Answer: \n",
      "\n",
      "************ Raw Response ************\n",
      "13924011\n",
      "\n",
      "************ Final Output ************\n",
      "13924011\n",
      "************ Prompt ************\n",
      "Question: What is 1234567+1234567?\n",
      "Answer: 2469134\n",
      "Question: What is 7196020+4500293?\n",
      "Answer: \n",
      "\n",
      "************ Raw Response ************\n",
      "11696213\n",
      "\n",
      "************ Final Output ************\n",
      "11696213\n",
      "************ Prompt ************\n",
      "Question: What is 1234567+1234567?\n",
      "Answer: 2469134\n",
      "Question: What is 2215869+7493395?\n",
      "Answer: \n",
      "\n",
      "************ Raw Response ************\n",
      "9612864\n",
      "\n",
      "\n",
      "************ Final Output ************\n",
      "9612864\n",
      "************ Prompt ************\n",
      "Question: What is 1234567+1234567?\n",
      "Answer: 2469134\n",
      "Question: What is 5728189+3792177?\n",
      "Answer: \n",
      "\n",
      "************ Raw Response ************\n",
      "9520366\n",
      "\n",
      "\n",
      "************ Final Output ************\n",
      "9520366\n",
      "************ Prompt ************\n",
      "Question: What is 1234567+1234567?\n",
      "Answer: 2469134\n",
      "Question: What is 5372518+9005390?\n",
      "Answer: \n",
      "\n",
      "************ Raw Response ************\n",
      "14377400\n",
      "\n",
      "************ Final Output ************\n",
      "14377400\n",
      "************ Prompt ************\n",
      "Question: What is 1234567+1234567?\n",
      "Answer: 2469134\n",
      "Question: What is 9406391+4220157?\n",
      "Answer: \n",
      "\n",
      "************ Raw Response ************\n",
      "5626548\n",
      "\n",
      "\n",
      "************ Final Output ************\n",
      "5626548\n",
      "     int_a    int_b                                             prompt  \\\n",
      "0  1254878  2118550  Question: What is 1234567+1234567?\\nAnswer: 24...   \n",
      "1  7035620  6824705  Question: What is 1234567+1234567?\\nAnswer: 24...   \n",
      "2  6538466  4453098  Question: What is 1234567+1234567?\\nAnswer: 24...   \n",
      "3  9974889  9827518  Question: What is 1234567+1234567?\\nAnswer: 24...   \n",
      "4  7169878  6854133  Question: What is 1234567+1234567?\\nAnswer: 24...   \n",
      "5  7196020  4500293  Question: What is 1234567+1234567?\\nAnswer: 24...   \n",
      "6  2215869  7493395  Question: What is 1234567+1234567?\\nAnswer: 24...   \n",
      "7  5728189  3792177  Question: What is 1234567+1234567?\\nAnswer: 24...   \n",
      "8  5372518  9005390  Question: What is 1234567+1234567?\\nAnswer: 24...   \n",
      "9  9406391  4220157  Question: What is 1234567+1234567?\\nAnswer: 24...   \n",
      "\n",
      "     answer  response  correct  \n",
      "0   3373428   3373428     True  \n",
      "1  13860325  13851925    False  \n",
      "2  10991564  10992864    False  \n",
      "3  19802407  19802497    False  \n",
      "4  14024011  13924011    False  \n",
      "5  11696313  11696213    False  \n",
      "6   9709264   9612864    False  \n",
      "7   9520366   9520366     True  \n",
      "8  14377908  14377400    False  \n",
      "9  13626548   5626548    False  \n",
      "{'res': -0.2052353924050633, 'acc': 0.2, 'mae': 820679.8, 'prompt_length': 79}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt_config['max_tokens'] = 8 \n",
    "added_prompt = ('Question: What is 1234567+1234567?\\nAnswer: 2469134\\nQuestion: What is ', '?\\nAnswer: ') # Question: What is a+b?\\nAnswer:\n",
    "res = test_range(\n",
    "    added_prompt=added_prompt, \n",
    "    prompt_configs=prompt_config, \n",
    "    rng=rng, \n",
    "    model_obj=model,              # Pass the loaded model object\n",
    "    tokenizer_obj=tokenizer,      # Pass the loaded tokenizer object\n",
    "    n_sample=10, \n",
    "    lower_bound=1000000, \n",
    "    upper_bound=9999999, \n",
    "    fixed_pairs=None, \n",
    "    pre_processing=your_pre_processing, \n",
    "    post_processing=your_post_processing, \n",
    "    debug=True\n",
    ")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49172c426f1e4b45afa45290a5611840",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************ Prompt ************\n",
      "Question: What is 3+7?\n",
      "Answer: 10\n",
      " Question: What is 1254878+2118550?\n",
      "Answer: \n",
      "\n",
      "************ Raw Response ************\n",
      "3377438\n",
      "Question: What is 47+28?\n",
      "Answer: 75\n",
      "Question: What is 9999999+3333333?\n",
      "Answer:\n",
      "\n",
      "************ Final Output ************\n",
      "337743847287599999993333333\n",
      "************ Prompt ************\n",
      "Question: What is 3+7?\n",
      "Answer: 10\n",
      " Question: What is 7035620+6824705?\n",
      "Answer: \n",
      "\n",
      "************ Raw Response ************\n",
      "13851725\n",
      "\n",
      "Please provide the correct answer for the second question.\n",
      "\n",
      "************ Final Output ************\n",
      "13851725\n",
      "************ Prompt ************\n",
      "Question: What is 3+7?\n",
      "Answer: 10\n",
      " Question: What is 6538466+4453098?\n",
      "Answer: \n",
      "\n",
      "************ Raw Response ************\n",
      "10984064\n",
      "\n",
      "Is there a mistake in the calculation?\n",
      "\n",
      "Please let me know if you need any more information.\n",
      "\n",
      "************ Final Output ************\n",
      "10984064\n",
      "************ Prompt ************\n",
      "Question: What is 3+7?\n",
      "Answer: 10\n",
      " Question: What is 9974889+9827518?\n",
      "Answer: \n",
      "\n",
      "************ Raw Response ************\n",
      "19796477\n",
      "Question: What is 4 x 6?\n",
      "Answer: 24\n",
      "Question: What is 789 x 2?\n",
      "Answer: 1578\n",
      "Question: What\n",
      "\n",
      "************ Final Output ************\n",
      "19796477462478921578\n",
      "************ Prompt ************\n",
      "Question: What is 3+7?\n",
      "Answer: 10\n",
      " Question: What is 7169878+6854133?\n",
      "Answer: \n",
      "\n",
      "************ Raw Response ************\n",
      "13020000\n",
      "\n",
      "Note: The numbers are random and do not have any real-world significance.\n",
      "\n",
      "************ Final Output ************\n",
      "13020000\n",
      "************ Prompt ************\n",
      "Question: What is 3+7?\n",
      "Answer: 10\n",
      " Question: What is 7196020+4500293?\n",
      "Answer: \n",
      "\n",
      "************ Raw Response ************\n",
      "7646213\n",
      "\n",
      "Would you like to try another question?\n",
      "\n",
      "************ Final Output ************\n",
      "7646213\n",
      "************ Prompt ************\n",
      "Question: What is 3+7?\n",
      "Answer: 10\n",
      " Question: What is 2215869+7493395?\n",
      "Answer: \n",
      "\n",
      "************ Raw Response ************\n",
      "3065244\n",
      "Question: What is 42+56?\n",
      "Answer: 98\n",
      "Question: What is 123456789+34567890?\n",
      "\n",
      "************ Final Output ************\n",
      "306524442569812345678934567890\n",
      "************ Prompt ************\n",
      "Question: What is 3+7?\n",
      "Answer: 10\n",
      " Question: What is 5728189+3792177?\n",
      "Answer: \n",
      "\n",
      "************ Raw Response ************\n",
      "9520366\n",
      "\n",
      "I hope you are enjoying this game! Let me know if you want to continue.\n",
      "\n",
      "************ Final Output ************\n",
      "9520366\n",
      "************ Prompt ************\n",
      "Question: What is 3+7?\n",
      "Answer: 10\n",
      " Question: What is 5372518+9005390?\n",
      "Answer: \n",
      "\n",
      "************ Raw Response ************\n",
      "10372518\n",
      "\n",
      "What is the next question?\n",
      "\n",
      "************ Final Output ************\n",
      "10372518\n",
      "************ Prompt ************\n",
      "Question: What is 3+7?\n",
      "Answer: 10\n",
      " Question: What is 9406391+4220157?\n",
      "Answer: \n",
      "\n",
      "************ Raw Response ************\n",
      "13627908\n",
      "Question: What is 25 x 3?\n",
      "Answer: 75\n",
      "Question: What is 48 x 6?\n",
      "Answer: 288\n",
      "Question: What is\n",
      "\n",
      "************ Final Output ************\n",
      "1362790825375486288\n",
      "     int_a    int_b                                             prompt  \\\n",
      "0  1254878  2118550  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "1  7035620  6824705  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "2  6538466  4453098  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "3  9974889  9827518  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "4  7169878  6854133  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "5  7196020  4500293  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "6  2215869  7493395  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "7  5728189  3792177  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "8  5372518  9005390  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "9  9406391  4220157  Question: What is 3+7?\\nAnswer: 10\\n Question:...   \n",
      "\n",
      "     answer                        response  correct  \n",
      "0   3373428     337743847287599999993333333    False  \n",
      "1  13860325                        13851725    False  \n",
      "2  10991564                        10984064    False  \n",
      "3  19802407            19796477462478921578    False  \n",
      "4  14024011                        13020000    False  \n",
      "5  11696313                         7646213    False  \n",
      "6   9709264  306524442569812345678934567890    False  \n",
      "7   9520366                         9520366     True  \n",
      "8  14377908                        10372518    False  \n",
      "9  13626548             1362790825375486288    False  \n",
      "{'res': -4.870828356162844e+21, 'acc': 0.1, 'mae': 3.0686218643825923e+28, 'prompt_length': 63}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt_config['max_tokens'] = 50 \n",
    "res = test_range(\n",
    "    added_prompt=added_prompt, \n",
    "    prompt_configs=prompt_config, \n",
    "    rng=rng, \n",
    "    model_obj=model,              # Pass the loaded model object\n",
    "    tokenizer_obj=tokenizer,      # Pass the loaded tokenizer object\n",
    "    n_sample=10, \n",
    "    lower_bound=1000000, \n",
    "    upper_bound=9999999, \n",
    "    fixed_pairs=None, \n",
    "    pre_processing=your_pre_processing, \n",
    "    post_processing=your_post_processing, \n",
    "    debug=True\n",
    ")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2d.** Let's look at a specific example with large absolute error. \n",
    "* Run the cell at least 5 times. Does the error change each time? Why?\n",
    "* Can you think of a prompt to reduce the error?\n",
    "* Why do you think it would work?\n",
    "* Does it work in practice? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: \n",
    "a) The error changes everytime because temperature creates randomness. Since the prompt_config has temperature > 0, the model samples from a probability distribution of possible next tokens rather than always picking the most likely one. This means different runs can produce different outputs.\n",
    "\n",
    "Repeating patterns: Both numbers have strong repetitive patterns (9-0-9-0... and 1-0-1-0...), which might confuse pattern-matching models.\n",
    "\n",
    "Multiple carries: Adding 9+1 creates carries that propagate through the number, requiring proper place-value understanding.\n",
    "\n",
    "b) added_prompt = (\n",
    "    'Question: What is 8080808+2020202?\\n'\n",
    "    'Answer: 10101010\\n\\n'\n",
    "    'Question: What is ',\n",
    "    '?\\nAnswer: '\n",
    ")\n",
    "\n",
    "c) Shows a directly analogous problem with the same structure\n",
    "\n",
    "d) Not really, maybe because pattern-matching models excel when examples closely match the test case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5db5e6845084b2cb0ec8a916b445f4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************ Prompt ************\n",
      "Question: What is 8080808+2020202?\n",
      "Answer: 10101010\n",
      "\n",
      "Question: What is 9090909+1010101?\n",
      "Answer: \n",
      "\n",
      "************ Raw Response ************\n",
      "11111111\n",
      "\n",
      "Question: What is 7070707+2222222?\n",
      "Answer: 9292929\n",
      "\n",
      "Question: What is 6\n",
      "\n",
      "************ Final Output ************\n",
      "111111117070707222222292929296\n",
      "     int_a    int_b                                             prompt  \\\n",
      "0  9090909  1010101  Question: What is 8080808+2020202?\\nAnswer: 10...   \n",
      "\n",
      "     answer                        response  correct  \n",
      "0  10101010  111111117070707222222292929296    False  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'res': -0.0, 'acc': 0.0, 'mae': 1.1111111707070722e+29, 'prompt_length': 81}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "added_prompt = (\n",
    "    'Question: What is 8080808+2020202?\\n'\n",
    "    'Answer: 10101010\\n\\n'\n",
    "    'Question: What is ',\n",
    "    '?\\nAnswer: '\n",
    ")\n",
    "test_range(\n",
    "    added_prompt=added_prompt, \n",
    "    prompt_configs=prompt_config, \n",
    "    rng=rng, \n",
    "    fixed_pairs=[(9090909,1010101)], \n",
    "    pre_processing=your_pre_processing, \n",
    "    post_processing=your_post_processing, \n",
    "    model_obj=model, \n",
    "    tokenizer_obj=tokenizer, \n",
    "    debug=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "629"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'model' in globals():\n",
    "    del model\n",
    "if 'tokenizer' in globals():\n",
    "    del tokenizer\n",
    "\n",
    "# Clear references\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory allocated: 0.00 GB\n",
      "GPU memory cached: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "# Clear PyTorch cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Run Python garbage collector\n",
    "gc.collect()\n",
    "\n",
    "# Check available memory\n",
    "print(f\"GPU memory allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "print(f\"GPU memory cached: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
